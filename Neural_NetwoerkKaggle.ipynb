{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soichiro-Gardinner/Neural_Networks/blob/main/Neural_NetwoerkKaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hausing Dataset**\n",
        "- **By:** Oscar Castanaza"
      ],
      "metadata": {
        "id": "zjwXcMs5k86i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idd1ybE-dCrG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "df.head(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "OGR8X6XWeNpI",
        "outputId": "fc9941fd-0457-425d-a6b9-23a7c65e1f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0    1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1    2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2    3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3    4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4    5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "5    6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
              "6    7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
              "7    8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
              "8    9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
              "9   10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
              "10  11          20       RL         70.0    11200   Pave   NaN      Reg   \n",
              "\n",
              "   LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
              "0          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "1          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "2          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "3          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "4          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "5          Lvl    AllPub  ...        0    NaN  MnPrv        Shed     700   \n",
              "6          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "7          Lvl    AllPub  ...        0    NaN    NaN        Shed     350   \n",
              "8          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "9          Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "10         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
              "\n",
              "   MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0       2   2008        WD         Normal     208500  \n",
              "1       5   2007        WD         Normal     181500  \n",
              "2       9   2008        WD         Normal     223500  \n",
              "3       2   2006        WD        Abnorml     140000  \n",
              "4      12   2008        WD         Normal     250000  \n",
              "5      10   2009        WD         Normal     143000  \n",
              "6       8   2007        WD         Normal     307000  \n",
              "7      11   2009        WD         Normal     200000  \n",
              "8       4   2008        WD        Abnorml     129900  \n",
              "9       1   2008        WD         Normal     118000  \n",
              "10      2   2008        WD         Normal     129500  \n",
              "\n",
              "[11 rows x 81 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d39459b-97ad-4668-b134-89ad53bcc7a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>50</td>\n",
              "      <td>RL</td>\n",
              "      <td>85.0</td>\n",
              "      <td>14115</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MnPrv</td>\n",
              "      <td>Shed</td>\n",
              "      <td>700</td>\n",
              "      <td>10</td>\n",
              "      <td>2009</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>143000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>75.0</td>\n",
              "      <td>10084</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10382</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Shed</td>\n",
              "      <td>350</td>\n",
              "      <td>11</td>\n",
              "      <td>2009</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>50</td>\n",
              "      <td>RM</td>\n",
              "      <td>51.0</td>\n",
              "      <td>6120</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>129900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>190</td>\n",
              "      <td>RL</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7420</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>70.0</td>\n",
              "      <td>11200</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>129500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows Ã— 81 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d39459b-97ad-4668-b134-89ad53bcc7a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d39459b-97ad-4668-b134-89ad53bcc7a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d39459b-97ad-4668-b134-89ad53bcc7a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUfwe88cjjQc",
        "outputId": "4d60aea1-4bb6-45c4-e402-73bc24da7262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                 0\n",
              "MSSubClass         0\n",
              "MSZoning           0\n",
              "LotFrontage      259\n",
              "LotArea            0\n",
              "                ... \n",
              "MoSold             0\n",
              "YrSold             0\n",
              "SaleType           0\n",
              "SaleCondition      0\n",
              "SalePrice          0\n",
              "Length: 81, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh5tu08Ojo9N",
        "outputId": "fea73f8e-4026-4d8e-b989-c9750dcf8b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 81)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split and Impute**"
      ],
      "metadata": {
        "id": "m_ZJV5WtlSke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Impute"
      ],
      "metadata": {
        "id": "7O8kOalvmiTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Separate the features (X) and target variable (y)\n",
        "X = df.drop('SalePrice', axis=1)\n",
        "y = df['SalePrice']\n",
        "\n",
        "# Identify the numeric and categorical columns\n",
        "numeric_cols = X.select_dtypes(include='number').columns\n",
        "categorical_cols = X.select_dtypes(include='object').columns\n",
        "\n",
        "# Impute missing values in numeric columns with mean imputation\n",
        "numeric_imputer = SimpleImputer(strategy='mean')\n",
        "X[numeric_cols] = numeric_imputer.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Impute missing values in categorical columns with mode imputation\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X[categorical_cols] = categorical_imputer.fit_transform(X[categorical_cols])\n",
        "\n",
        "# Split the imputed data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "EuNXUgb4lkWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scale, OH encoder, ColumnTransformer and Pipeline.**"
      ],
      "metadata": {
        "id": "CO9U5sCbwwGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Preprocessing pipeline for numeric features\n",
        "numeric_features = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
        "                    'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n",
        "                    '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
        "                    'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n",
        "                    'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
        "                    'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing pipeline for categorical features\n",
        "categorical_features = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
        "                        'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
        "                        'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
        "                        'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
        "                        'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                        'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
        "                        'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n",
        "                        'SaleCondition']\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Create a preprocessor to apply the transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply the preprocessing to the training and test data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "-4iD7VIKwvvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "PlmlHQsBl-4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 1"
      ],
      "metadata": {
        "id": "4Y5nfFaj2mg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Define the architecture of the deep learning model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_preprocessed.toarray(), y_train, epochs=100, batch_size=32, validation_data=(X_test_preprocessed.toarray(), y_test))\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss = model.evaluate(X_test_preprocessed.toarray(), y_test.values)\n",
        "print('\\nMean Squared Error:', loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGNbcn4J0o5a",
        "outputId": "f99a3f4e-cb7c-46c6-fe82-724047eac1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "37/37 [==============================] - 2s 15ms/step - loss: 38883160064.0000 - val_loss: 39645880320.0000\n",
            "Epoch 2/100\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 38858674176.0000 - val_loss: 39594762240.0000\n",
            "Epoch 3/100\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 38757244928.0000 - val_loss: 39428358144.0000\n",
            "Epoch 4/100\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 38496329728.0000 - val_loss: 39054110720.0000\n",
            "Epoch 5/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 37974491136.0000 - val_loss: 38376656896.0000\n",
            "Epoch 6/100\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 37092888576.0000 - val_loss: 37296721920.0000\n",
            "Epoch 7/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 35791855616.0000 - val_loss: 35763359744.0000\n",
            "Epoch 8/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 33983461376.0000 - val_loss: 33747650560.0000\n",
            "Epoch 9/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 31713212416.0000 - val_loss: 31250634752.0000\n",
            "Epoch 10/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 28994576384.0000 - val_loss: 28291899392.0000\n",
            "Epoch 11/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 25773852672.0000 - val_loss: 24996462592.0000\n",
            "Epoch 12/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 22308415488.0000 - val_loss: 21483145216.0000\n",
            "Epoch 13/100\n",
            "37/37 [==============================] - 0s 12ms/step - loss: 18881945600.0000 - val_loss: 17894862848.0000\n",
            "Epoch 14/100\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 15250791424.0000 - val_loss: 14438891520.0000\n",
            "Epoch 15/100\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 12028362752.0000 - val_loss: 11333022720.0000\n",
            "Epoch 16/100\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 9019894784.0000 - val_loss: 8756730880.0000\n",
            "Epoch 17/100\n",
            "37/37 [==============================] - 1s 17ms/step - loss: 6957281792.0000 - val_loss: 6755163136.0000\n",
            "Epoch 18/100\n",
            "37/37 [==============================] - 1s 16ms/step - loss: 5154201088.0000 - val_loss: 5297375744.0000\n",
            "Epoch 19/100\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 4060546816.0000 - val_loss: 4288266496.0000\n",
            "Epoch 20/100\n",
            "37/37 [==============================] - 1s 15ms/step - loss: 3326479104.0000 - val_loss: 3658412800.0000\n",
            "Epoch 21/100\n",
            "37/37 [==============================] - 1s 21ms/step - loss: 2990532352.0000 - val_loss: 3232157952.0000\n",
            "Epoch 22/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2615100928.0000 - val_loss: 2961150208.0000\n",
            "Epoch 23/100\n",
            "37/37 [==============================] - 0s 9ms/step - loss: 2417727488.0000 - val_loss: 2782587136.0000\n",
            "Epoch 24/100\n",
            "37/37 [==============================] - 0s 10ms/step - loss: 2388625920.0000 - val_loss: 2616647680.0000\n",
            "Epoch 25/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2349662976.0000 - val_loss: 2509096960.0000\n",
            "Epoch 26/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 2266294272.0000 - val_loss: 2402159104.0000\n",
            "Epoch 27/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 2272673280.0000 - val_loss: 2326104576.0000\n",
            "Epoch 28/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 2006362240.0000 - val_loss: 2240760576.0000\n",
            "Epoch 29/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 2218552832.0000 - val_loss: 2166762240.0000\n",
            "Epoch 30/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1992234880.0000 - val_loss: 2102337152.0000\n",
            "Epoch 31/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1985548672.0000 - val_loss: 2040007296.0000\n",
            "Epoch 32/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1998012416.0000 - val_loss: 1989520384.0000\n",
            "Epoch 33/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1904552192.0000 - val_loss: 1937573376.0000\n",
            "Epoch 34/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1938987776.0000 - val_loss: 1897048576.0000\n",
            "Epoch 35/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1920640000.0000 - val_loss: 1864986624.0000\n",
            "Epoch 36/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1930688256.0000 - val_loss: 1815499392.0000\n",
            "Epoch 37/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1779737984.0000 - val_loss: 1792624512.0000\n",
            "Epoch 38/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1881645056.0000 - val_loss: 1765937408.0000\n",
            "Epoch 39/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1643134336.0000 - val_loss: 1726719360.0000\n",
            "Epoch 40/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1783791488.0000 - val_loss: 1707905920.0000\n",
            "Epoch 41/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1754774656.0000 - val_loss: 1681005312.0000\n",
            "Epoch 42/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1731088128.0000 - val_loss: 1665557504.0000\n",
            "Epoch 43/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1774826368.0000 - val_loss: 1648074880.0000\n",
            "Epoch 44/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1643678592.0000 - val_loss: 1630246016.0000\n",
            "Epoch 45/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1614735616.0000 - val_loss: 1608827136.0000\n",
            "Epoch 46/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1669435648.0000 - val_loss: 1585099520.0000\n",
            "Epoch 47/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1691453312.0000 - val_loss: 1572044032.0000\n",
            "Epoch 48/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1679606912.0000 - val_loss: 1564551168.0000\n",
            "Epoch 49/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1602574976.0000 - val_loss: 1542497664.0000\n",
            "Epoch 50/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1604443648.0000 - val_loss: 1527354112.0000\n",
            "Epoch 51/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1591442944.0000 - val_loss: 1516928128.0000\n",
            "Epoch 52/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1614004736.0000 - val_loss: 1511202816.0000\n",
            "Epoch 53/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1508546816.0000 - val_loss: 1482676864.0000\n",
            "Epoch 54/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1564572288.0000 - val_loss: 1491962496.0000\n",
            "Epoch 55/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1583484800.0000 - val_loss: 1470414848.0000\n",
            "Epoch 56/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1546314112.0000 - val_loss: 1431877120.0000\n",
            "Epoch 57/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1513067904.0000 - val_loss: 1438053248.0000\n",
            "Epoch 58/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1484375168.0000 - val_loss: 1423511168.0000\n",
            "Epoch 59/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1408204288.0000 - val_loss: 1420191104.0000\n",
            "Epoch 60/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1598664576.0000 - val_loss: 1428810496.0000\n",
            "Epoch 61/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1463351168.0000 - val_loss: 1395375104.0000\n",
            "Epoch 62/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1450079872.0000 - val_loss: 1385410304.0000\n",
            "Epoch 63/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1458229376.0000 - val_loss: 1383352576.0000\n",
            "Epoch 64/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1568406144.0000 - val_loss: 1370526720.0000\n",
            "Epoch 65/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1478241152.0000 - val_loss: 1375739904.0000\n",
            "Epoch 66/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1469252352.0000 - val_loss: 1359834368.0000\n",
            "Epoch 67/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1544940160.0000 - val_loss: 1355915776.0000\n",
            "Epoch 68/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1459661568.0000 - val_loss: 1348160640.0000\n",
            "Epoch 69/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1526181760.0000 - val_loss: 1338653952.0000\n",
            "Epoch 70/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1532782592.0000 - val_loss: 1334069504.0000\n",
            "Epoch 71/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1513336832.0000 - val_loss: 1332445696.0000\n",
            "Epoch 72/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1421153280.0000 - val_loss: 1313911040.0000\n",
            "Epoch 73/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1446112768.0000 - val_loss: 1326427008.0000\n",
            "Epoch 74/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1538062720.0000 - val_loss: 1295890048.0000\n",
            "Epoch 75/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1421793792.0000 - val_loss: 1284897408.0000\n",
            "Epoch 76/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1363459712.0000 - val_loss: 1303367168.0000\n",
            "Epoch 77/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1367746048.0000 - val_loss: 1310605696.0000\n",
            "Epoch 78/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1515630464.0000 - val_loss: 1289738112.0000\n",
            "Epoch 79/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1501307008.0000 - val_loss: 1290494208.0000\n",
            "Epoch 80/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1330703360.0000 - val_loss: 1268076416.0000\n",
            "Epoch 81/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1447732736.0000 - val_loss: 1291199872.0000\n",
            "Epoch 82/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1401756288.0000 - val_loss: 1256934656.0000\n",
            "Epoch 83/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1400326400.0000 - val_loss: 1273763072.0000\n",
            "Epoch 84/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1528873344.0000 - val_loss: 1256882944.0000\n",
            "Epoch 85/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1431465600.0000 - val_loss: 1262204160.0000\n",
            "Epoch 86/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1537896192.0000 - val_loss: 1240611456.0000\n",
            "Epoch 87/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1468258560.0000 - val_loss: 1243169024.0000\n",
            "Epoch 88/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1394130304.0000 - val_loss: 1222165376.0000\n",
            "Epoch 89/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1196241664.0000 - val_loss: 1228350848.0000\n",
            "Epoch 90/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1412157440.0000 - val_loss: 1225243264.0000\n",
            "Epoch 91/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1335437440.0000 - val_loss: 1230874880.0000\n",
            "Epoch 92/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1365077120.0000 - val_loss: 1218296832.0000\n",
            "Epoch 93/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1448732800.0000 - val_loss: 1233830784.0000\n",
            "Epoch 94/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1454062080.0000 - val_loss: 1221871744.0000\n",
            "Epoch 95/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1441848704.0000 - val_loss: 1208483328.0000\n",
            "Epoch 96/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1374287104.0000 - val_loss: 1215784832.0000\n",
            "Epoch 97/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1417143680.0000 - val_loss: 1195568896.0000\n",
            "Epoch 98/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1379188608.0000 - val_loss: 1210469248.0000\n",
            "Epoch 99/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1394733312.0000 - val_loss: 1203456640.0000\n",
            "Epoch 100/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1368210944.0000 - val_loss: 1201398272.0000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1201398272.0000\n",
            "\n",
            "Mean Squared Error: 1201398272.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 2"
      ],
      "metadata": {
        "id": "8c4lbzKz21kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture of the deep learning model\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],), kernel_regularizer=regularizers.l2(0.001)))\n",
        "model2.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model2.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history2 = model2.fit(X_train_preprocessed.toarray(), y_train.values, epochs=100, batch_size=32, validation_data=(X_test_preprocessed.toarray(), y_test.values))\n",
        "\n",
        "# Evaluate the model\n",
        "loss2 = model2.evaluate(X_test_preprocessed.toarray(), y_test.values)\n",
        "print('Model 2 - Mean Squared Error:', loss2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc7ty7br2wbS",
        "outputId": "3be05c92-31e2-48c9-ceb2-50062568b220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "37/37 [==============================] - 1s 7ms/step - loss: 38883889152.0000 - val_loss: 39648301056.0000\n",
            "Epoch 2/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 38866239488.0000 - val_loss: 39610933248.0000\n",
            "Epoch 3/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 38792368128.0000 - val_loss: 39486771200.0000\n",
            "Epoch 4/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 38593736704.0000 - val_loss: 39204347904.0000\n",
            "Epoch 5/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 38195728384.0000 - val_loss: 38679236608.0000\n",
            "Epoch 6/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 37509959680.0000 - val_loss: 37835579392.0000\n",
            "Epoch 7/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 36471656448.0000 - val_loss: 36615856128.0000\n",
            "Epoch 8/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 35035824128.0000 - val_loss: 34981924864.0000\n",
            "Epoch 9/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 33173878784.0000 - val_loss: 32919932928.0000\n",
            "Epoch 10/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 30881378304.0000 - val_loss: 30459602944.0000\n",
            "Epoch 11/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 28196608000.0000 - val_loss: 27660419072.0000\n",
            "Epoch 12/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 25226885120.0000 - val_loss: 24553678848.0000\n",
            "Epoch 13/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 22007230464.0000 - val_loss: 21344303104.0000\n",
            "Epoch 14/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18726801408.0000 - val_loss: 18076637184.0000\n",
            "Epoch 15/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15493184512.0000 - val_loss: 14967326720.0000\n",
            "Epoch 16/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 12493285376.0000 - val_loss: 12084385792.0000\n",
            "Epoch 17/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 9808899072.0000 - val_loss: 9617470464.0000\n",
            "Epoch 18/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 7562485248.0000 - val_loss: 7593596928.0000\n",
            "Epoch 19/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 5788923904.0000 - val_loss: 6006818816.0000\n",
            "Epoch 20/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 4449410048.0000 - val_loss: 4852259328.0000\n",
            "Epoch 21/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 3516170496.0000 - val_loss: 4040041728.0000\n",
            "Epoch 22/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2902153472.0000 - val_loss: 3474366464.0000\n",
            "Epoch 23/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2510523392.0000 - val_loss: 3108727552.0000\n",
            "Epoch 24/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 2267635712.0000 - val_loss: 2858777088.0000\n",
            "Epoch 25/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2107869184.0000 - val_loss: 2683326464.0000\n",
            "Epoch 26/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1995629952.0000 - val_loss: 2546280704.0000\n",
            "Epoch 27/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1910798976.0000 - val_loss: 2433798656.0000\n",
            "Epoch 28/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1843459968.0000 - val_loss: 2338109184.0000\n",
            "Epoch 29/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1785760000.0000 - val_loss: 2252826880.0000\n",
            "Epoch 30/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1733872512.0000 - val_loss: 2185114880.0000\n",
            "Epoch 31/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1690321792.0000 - val_loss: 2115116032.0000\n",
            "Epoch 32/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1646871296.0000 - val_loss: 2062963712.0000\n",
            "Epoch 33/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1610776192.0000 - val_loss: 2008913664.0000\n",
            "Epoch 34/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1576924800.0000 - val_loss: 1957944960.0000\n",
            "Epoch 35/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1546190464.0000 - val_loss: 1917461760.0000\n",
            "Epoch 36/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1519451136.0000 - val_loss: 1875191296.0000\n",
            "Epoch 37/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1497099776.0000 - val_loss: 1845296256.0000\n",
            "Epoch 38/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1474578176.0000 - val_loss: 1802942464.0000\n",
            "Epoch 39/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1449976448.0000 - val_loss: 1778697216.0000\n",
            "Epoch 40/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1431284608.0000 - val_loss: 1745821184.0000\n",
            "Epoch 41/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1412423680.0000 - val_loss: 1717950720.0000\n",
            "Epoch 42/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1396038272.0000 - val_loss: 1698902016.0000\n",
            "Epoch 43/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1379389952.0000 - val_loss: 1675766912.0000\n",
            "Epoch 44/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1364539264.0000 - val_loss: 1655489280.0000\n",
            "Epoch 45/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1350393600.0000 - val_loss: 1632299520.0000\n",
            "Epoch 46/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1336942080.0000 - val_loss: 1614064768.0000\n",
            "Epoch 47/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1326265216.0000 - val_loss: 1587363328.0000\n",
            "Epoch 48/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1310929280.0000 - val_loss: 1578216960.0000\n",
            "Epoch 49/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1302189824.0000 - val_loss: 1567559168.0000\n",
            "Epoch 50/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1289089280.0000 - val_loss: 1543313024.0000\n",
            "Epoch 51/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1280816768.0000 - val_loss: 1536337664.0000\n",
            "Epoch 52/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1271232640.0000 - val_loss: 1510449920.0000\n",
            "Epoch 53/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1257616640.0000 - val_loss: 1508785024.0000\n",
            "Epoch 54/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1249410688.0000 - val_loss: 1490454144.0000\n",
            "Epoch 55/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1238650752.0000 - val_loss: 1476283776.0000\n",
            "Epoch 56/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1229876096.0000 - val_loss: 1462059392.0000\n",
            "Epoch 57/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1220068096.0000 - val_loss: 1453770112.0000\n",
            "Epoch 58/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1211625984.0000 - val_loss: 1443209984.0000\n",
            "Epoch 59/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1203964416.0000 - val_loss: 1435365888.0000\n",
            "Epoch 60/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1199441408.0000 - val_loss: 1424551936.0000\n",
            "Epoch 61/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1188248320.0000 - val_loss: 1405583104.0000\n",
            "Epoch 62/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1181019264.0000 - val_loss: 1400321536.0000\n",
            "Epoch 63/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1173131648.0000 - val_loss: 1397382144.0000\n",
            "Epoch 64/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1167453312.0000 - val_loss: 1389731712.0000\n",
            "Epoch 65/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1159420288.0000 - val_loss: 1373827968.0000\n",
            "Epoch 66/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1151790720.0000 - val_loss: 1368145408.0000\n",
            "Epoch 67/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1144978176.0000 - val_loss: 1360719872.0000\n",
            "Epoch 68/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1141139072.0000 - val_loss: 1343727232.0000\n",
            "Epoch 69/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1133888384.0000 - val_loss: 1347839488.0000\n",
            "Epoch 70/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1125240960.0000 - val_loss: 1337952640.0000\n",
            "Epoch 71/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1119698560.0000 - val_loss: 1330441472.0000\n",
            "Epoch 72/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1112898688.0000 - val_loss: 1322211712.0000\n",
            "Epoch 73/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1107241216.0000 - val_loss: 1313576832.0000\n",
            "Epoch 74/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1101054208.0000 - val_loss: 1306149760.0000\n",
            "Epoch 75/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1095565824.0000 - val_loss: 1294932736.0000\n",
            "Epoch 76/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1089617792.0000 - val_loss: 1297067136.0000\n",
            "Epoch 77/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1086013568.0000 - val_loss: 1290385152.0000\n",
            "Epoch 78/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1081047040.0000 - val_loss: 1289576832.0000\n",
            "Epoch 79/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1077031040.0000 - val_loss: 1269876736.0000\n",
            "Epoch 80/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1070558080.0000 - val_loss: 1274173056.0000\n",
            "Epoch 81/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1064224192.0000 - val_loss: 1268258816.0000\n",
            "Epoch 82/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1059407424.0000 - val_loss: 1255537792.0000\n",
            "Epoch 83/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1055664192.0000 - val_loss: 1249700224.0000\n",
            "Epoch 84/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1048948544.0000 - val_loss: 1242613504.0000\n",
            "Epoch 85/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1047408576.0000 - val_loss: 1241404288.0000\n",
            "Epoch 86/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1040191680.0000 - val_loss: 1238814080.0000\n",
            "Epoch 87/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1035890432.0000 - val_loss: 1223992448.0000\n",
            "Epoch 88/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1030582912.0000 - val_loss: 1221998336.0000\n",
            "Epoch 89/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1027350208.0000 - val_loss: 1218940672.0000\n",
            "Epoch 90/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1023566976.0000 - val_loss: 1216594560.0000\n",
            "Epoch 91/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1018739264.0000 - val_loss: 1211171584.0000\n",
            "Epoch 92/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1014519488.0000 - val_loss: 1200546432.0000\n",
            "Epoch 93/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1009959488.0000 - val_loss: 1204229760.0000\n",
            "Epoch 94/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 1007149952.0000 - val_loss: 1201317248.0000\n",
            "Epoch 95/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1002892352.0000 - val_loss: 1187816960.0000\n",
            "Epoch 96/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 999467904.0000 - val_loss: 1192546560.0000\n",
            "Epoch 97/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 995054528.0000 - val_loss: 1181705472.0000\n",
            "Epoch 98/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 991327744.0000 - val_loss: 1186300672.0000\n",
            "Epoch 99/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 989198912.0000 - val_loss: 1174647680.0000\n",
            "Epoch 100/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 983630720.0000 - val_loss: 1170308992.0000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1170308992.0000\n",
            "Model 2 - Mean Squared Error: 1170308992.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Model 3:** Adding L2 Regularization (Ridge Regression)"
      ],
      "metadata": {
        "id": "xNxlE6q13AFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "# Define the architecture of the deep learning model\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],), kernel_regularizer=regularizers.l2(0.001)))\n",
        "model3.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model3.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model3.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history3 = model3.fit(X_train_preprocessed.toarray(), y_train.values, epochs=100, batch_size=32, validation_data=(X_test_preprocessed.toarray(), y_test.values))\n",
        "\n",
        "# Evaluate the model\n",
        "loss3 = model3.evaluate(X_test_preprocessed.toarray(), y_test.values)\n",
        "print('Model 3 - Mean Squared Error:', loss3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JEoSI4o3IB_",
        "outputId": "dbd0eba1-8900-421c-d7b7-33dad741a92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "37/37 [==============================] - 1s 13ms/step - loss: 38882889728.0000 - val_loss: 39645114368.0000\n",
            "Epoch 2/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 38856429568.0000 - val_loss: 39591661568.0000\n",
            "Epoch 3/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 38756343808.0000 - val_loss: 39429029888.0000\n",
            "Epoch 4/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 38503624704.0000 - val_loss: 39074435072.0000\n",
            "Epoch 5/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 38010658816.0000 - val_loss: 38437052416.0000\n",
            "Epoch 6/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 37197533184.0000 - val_loss: 37443158016.0000\n",
            "Epoch 7/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 35991040000.0000 - val_loss: 36040368128.0000\n",
            "Epoch 8/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 34347089920.0000 - val_loss: 34182881280.0000\n",
            "Epoch 9/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 32237412352.0000 - val_loss: 31895914496.0000\n",
            "Epoch 10/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 29703518208.0000 - val_loss: 29177430016.0000\n",
            "Epoch 11/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 26781542400.0000 - val_loss: 26100484096.0000\n",
            "Epoch 12/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 23552727040.0000 - val_loss: 22816458752.0000\n",
            "Epoch 13/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20168435712.0000 - val_loss: 19420979200.0000\n",
            "Epoch 14/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 16776465408.0000 - val_loss: 16127225856.0000\n",
            "Epoch 15/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 13540833280.0000 - val_loss: 13046310912.0000\n",
            "Epoch 16/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 10617844736.0000 - val_loss: 10322256896.0000\n",
            "Epoch 17/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 8130290176.0000 - val_loss: 8038300160.0000\n",
            "Epoch 18/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 6142486016.0000 - val_loss: 6264949248.0000\n",
            "Epoch 19/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 4657715200.0000 - val_loss: 4994979328.0000\n",
            "Epoch 20/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 3643456512.0000 - val_loss: 4078800896.0000\n",
            "Epoch 21/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 2961286400.0000 - val_loss: 3499734528.0000\n",
            "Epoch 22/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2536019456.0000 - val_loss: 3113259776.0000\n",
            "Epoch 23/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2269496576.0000 - val_loss: 2847053568.0000\n",
            "Epoch 24/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2095679872.0000 - val_loss: 2663098112.0000\n",
            "Epoch 25/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1982012160.0000 - val_loss: 2512126976.0000\n",
            "Epoch 26/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1895754240.0000 - val_loss: 2399887872.0000\n",
            "Epoch 27/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1826731136.0000 - val_loss: 2306513408.0000\n",
            "Epoch 28/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1768238976.0000 - val_loss: 2227349504.0000\n",
            "Epoch 29/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1716699136.0000 - val_loss: 2153734656.0000\n",
            "Epoch 30/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1671654528.0000 - val_loss: 2087820288.0000\n",
            "Epoch 31/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1630337408.0000 - val_loss: 2028613632.0000\n",
            "Epoch 32/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1593757696.0000 - val_loss: 1976216960.0000\n",
            "Epoch 33/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1561213056.0000 - val_loss: 1926809344.0000\n",
            "Epoch 34/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1531878400.0000 - val_loss: 1886921216.0000\n",
            "Epoch 35/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1504744832.0000 - val_loss: 1845448448.0000\n",
            "Epoch 36/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1478775680.0000 - val_loss: 1814765952.0000\n",
            "Epoch 37/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1459808512.0000 - val_loss: 1775669120.0000\n",
            "Epoch 38/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1436942720.0000 - val_loss: 1751566592.0000\n",
            "Epoch 39/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1416187264.0000 - val_loss: 1724141056.0000\n",
            "Epoch 40/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1399133440.0000 - val_loss: 1695972352.0000\n",
            "Epoch 41/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1383315072.0000 - val_loss: 1666721920.0000\n",
            "Epoch 42/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1366609280.0000 - val_loss: 1647371264.0000\n",
            "Epoch 43/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1353126400.0000 - val_loss: 1632802560.0000\n",
            "Epoch 44/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1339097344.0000 - val_loss: 1613262592.0000\n",
            "Epoch 45/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1326022528.0000 - val_loss: 1583122304.0000\n",
            "Epoch 46/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1316768256.0000 - val_loss: 1582011648.0000\n",
            "Epoch 47/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1301961856.0000 - val_loss: 1559504256.0000\n",
            "Epoch 48/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1289699328.0000 - val_loss: 1542728960.0000\n",
            "Epoch 49/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1278119168.0000 - val_loss: 1535404800.0000\n",
            "Epoch 50/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1267574272.0000 - val_loss: 1518583808.0000\n",
            "Epoch 51/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1258448768.0000 - val_loss: 1498043904.0000\n",
            "Epoch 52/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1245921792.0000 - val_loss: 1489587968.0000\n",
            "Epoch 53/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1237988864.0000 - val_loss: 1474546816.0000\n",
            "Epoch 54/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1228243328.0000 - val_loss: 1468747136.0000\n",
            "Epoch 55/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1218416640.0000 - val_loss: 1451339520.0000\n",
            "Epoch 56/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1209495040.0000 - val_loss: 1436513280.0000\n",
            "Epoch 57/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1201485184.0000 - val_loss: 1425886080.0000\n",
            "Epoch 58/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1193594624.0000 - val_loss: 1422287616.0000\n",
            "Epoch 59/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1187136768.0000 - val_loss: 1399512448.0000\n",
            "Epoch 60/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1177607424.0000 - val_loss: 1395682304.0000\n",
            "Epoch 61/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1168749184.0000 - val_loss: 1385376512.0000\n",
            "Epoch 62/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1164595328.0000 - val_loss: 1382785920.0000\n",
            "Epoch 63/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1154649856.0000 - val_loss: 1363229312.0000\n",
            "Epoch 64/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1148265600.0000 - val_loss: 1350859776.0000\n",
            "Epoch 65/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1141683456.0000 - val_loss: 1356157440.0000\n",
            "Epoch 66/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1133663616.0000 - val_loss: 1344373888.0000\n",
            "Epoch 67/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1127993216.0000 - val_loss: 1340552064.0000\n",
            "Epoch 68/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1121416960.0000 - val_loss: 1329394048.0000\n",
            "Epoch 69/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1115827072.0000 - val_loss: 1322811648.0000\n",
            "Epoch 70/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1108567296.0000 - val_loss: 1317324544.0000\n",
            "Epoch 71/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1101681792.0000 - val_loss: 1302530688.0000\n",
            "Epoch 72/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1096584192.0000 - val_loss: 1303646336.0000\n",
            "Epoch 73/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1090878080.0000 - val_loss: 1289024000.0000\n",
            "Epoch 74/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1086973696.0000 - val_loss: 1284303488.0000\n",
            "Epoch 75/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1081315328.0000 - val_loss: 1273388544.0000\n",
            "Epoch 76/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1074403456.0000 - val_loss: 1274704000.0000\n",
            "Epoch 77/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1069402112.0000 - val_loss: 1273263232.0000\n",
            "Epoch 78/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1062506560.0000 - val_loss: 1260180352.0000\n",
            "Epoch 79/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1059751104.0000 - val_loss: 1250027904.0000\n",
            "Epoch 80/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1054388864.0000 - val_loss: 1250721920.0000\n",
            "Epoch 81/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1049789568.0000 - val_loss: 1248366592.0000\n",
            "Epoch 82/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1047683392.0000 - val_loss: 1238738944.0000\n",
            "Epoch 83/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 1039862720.0000 - val_loss: 1225731712.0000\n",
            "Epoch 84/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1033557120.0000 - val_loss: 1227476480.0000\n",
            "Epoch 85/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1029864576.0000 - val_loss: 1225697024.0000\n",
            "Epoch 86/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1025561728.0000 - val_loss: 1218711424.0000\n",
            "Epoch 87/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1021282752.0000 - val_loss: 1211941248.0000\n",
            "Epoch 88/100\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1018307712.0000 - val_loss: 1202472064.0000\n",
            "Epoch 89/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1011857664.0000 - val_loss: 1204238208.0000\n",
            "Epoch 90/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1018973696.0000 - val_loss: 1197392384.0000\n",
            "Epoch 91/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 1004837760.0000 - val_loss: 1195725696.0000\n",
            "Epoch 92/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 1000472832.0000 - val_loss: 1182185472.0000\n",
            "Epoch 93/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 997854080.0000 - val_loss: 1188753408.0000\n",
            "Epoch 94/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 991707008.0000 - val_loss: 1179939712.0000\n",
            "Epoch 95/100\n",
            "37/37 [==============================] - 0s 7ms/step - loss: 989896256.0000 - val_loss: 1178831104.0000\n",
            "Epoch 96/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 985306688.0000 - val_loss: 1165694336.0000\n",
            "Epoch 97/100\n",
            "37/37 [==============================] - 0s 6ms/step - loss: 981249984.0000 - val_loss: 1168592000.0000\n",
            "Epoch 98/100\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 978747072.0000 - val_loss: 1173390208.0000\n",
            "Epoch 99/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 977690624.0000 - val_loss: 1154848512.0000\n",
            "Epoch 100/100\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 972758784.0000 - val_loss: 1162292864.0000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 1162292864.0000\n",
            "Model 3 - Mean Squared Error: 1162292864.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Select the Best Model:**"
      ],
      "metadata": {
        "id": "RBiWScTr393R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "print('Model 1 - Mean Squared Error:', loss)\n",
        "print('Model 2 - Mean Squared Error:', loss2)\n",
        "print('Model 3 - Mean Squared Error:', loss3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKfnNd5a39fu",
        "outputId": "59a68f7c-30ea-4bc3-bb00-5780da725c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 - Mean Squared Error: 1201398272.0\n",
            "Model 2 - Mean Squared Error: 1170308992.0\n",
            "Model 3 - Mean Squared Error: 1162292864.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 1:"
      ],
      "metadata": {
        "id": "5GckTmgq4_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred_1 = model.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_1 = mean_squared_error(y_test, y_pred_1)\n",
        "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
        "r2_1 = r2_score(y_test, y_pred_1)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Model 1 Metrics:\")\n",
        "print(\"Mean Squared Error:\", mse_1)\n",
        "print(\"Mean Absolute Error:\", mae_1)\n",
        "print(\"R-squared Score:\", r2_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-SkWujM5Dyj",
        "outputId": "cb772a08-70dc-4a2a-8c74-813d106bd157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 3ms/step\n",
            "Model 1 Metrics:\n",
            "Mean Squared Error: 1201398183.798757\n",
            "Mean Absolute Error: 19682.55070098459\n",
            "R-squared Score: 0.8433706531707814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 2:"
      ],
      "metadata": {
        "id": "rFFPnRtZ5GdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred_2 = model2.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_2 = mean_squared_error(y_test, y_pred_2)\n",
        "mae_2 = mean_absolute_error(y_test, y_pred_2)\n",
        "r2_2 = r2_score(y_test, y_pred_2)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Model 2 Metrics:\")\n",
        "print(\"Mean Squared Error:\", mse_2)\n",
        "print(\"Mean Absolute Error:\", mae_2)\n",
        "print(\"R-squared Score:\", r2_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G09Kj3qc5USP",
        "outputId": "8961d3af-9e51-4692-8258-59a11a21b48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Model 2 Metrics:\n",
            "Mean Squared Error: 1170309099.3228042\n",
            "Mean Absolute Error: 19644.836191673803\n",
            "R-squared Score: 0.8474238164439186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model 3:"
      ],
      "metadata": {
        "id": "_2EEeU-05RSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred_3 = model3.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_3 = mean_squared_error(y_test, y_pred_3)\n",
        "mae_3 = mean_absolute_error(y_test, y_pred_3)\n",
        "r2_3 = r2_score(y_test, y_pred_3)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Model 3 Metrics:\")\n",
        "print(\"Mean Squared Error:\", mse_3)\n",
        "print(\"Mean Absolute Error:\", mae_3)\n",
        "print(\"R-squared Score:\", r2_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXEq5PcM5jMd",
        "outputId": "2d185b6b-9b43-4ffd-ea6e-9054e16e796e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 4ms/step\n",
            "Model 3 Metrics:\n",
            "Mean Squared Error: 1162292830.3642085\n",
            "Mean Absolute Error: 19532.43358037243\n",
            "R-squared Score: 0.8484689178831618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selected**"
      ],
      "metadata": {
        "id": "TEAGwxZU7UdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the metrics, Model 3 has the lowest Mean Squared Error (MSE) among the three models:\n",
        "\n",
        "- Model 1: MSE = 1,201,398,183.80\n",
        "- Model 2: MSE = 1,170,309,099.32\n",
        "- Model 3: MSE = 1,162,292,830.36\n",
        "\n",
        "Lower MSE values indicate better model performance because it means the model's predictions are closer to the actual target values. In this case, Model 3 has the lowest MSE, suggesting that it has better predictive accuracy compared to the other two models.\n",
        "\n"
      ],
      "metadata": {
        "id": "5AbgUqdk7YSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Additionally**, the R-squared score is a measure of how well the model fits the data. Model 3 has the highest R-squared score of 0.848, which indicates that approximately 84.8% of the variance in the target variable can be explained by the model's predictions. Higher R-squared scores generally indicate a better fit to the data.\n",
        "\n",
        "## **Therefore, based on the lower MSE and higher R-squared score, Model 3 appears to be the best model among the three.**"
      ],
      "metadata": {
        "id": "q88-9k9s7z6q"
      }
    }
  ]
}